# Optimization

## 非凸优化

* 非凸优化基石：Lipschitz Condition - Zeap的文章 - 知乎 https://zhuanlan.zhihu.com/p/27554191

## 优化器差异

* [DL笔记] 从SGD到AdamW - 莫冉的文章 - 知乎 https://zhuanlan.zhihu.com/p/113112032
* https://www.fast.ai/2018/07/02/adam-weight-decay/
* 当前训练神经网络最快的方式：AdamW优化算法+超级收敛 - 机器之心的文章 - 知乎 https://zhuanlan.zhihu.com/p/38945390
* 都9102年了，别再用Adam + L2 regularization了 - paperplanet的文章 - 知乎 https://zhuanlan.zhihu.com/p/63982470

## 损失函数

* 何恺明大神的「Focal Loss」，如何更好地理解？ - PaperWeekly的文章 - 知乎 https://zhuanlan.zhihu.com/p/32423092
* [Focal Loss for Dense Object Detection in PyTorch](https://github.com/clcarwin/focal_loss_pytorch)
* triplet loss 损失函数 - 洪雨的文章 - 知乎 https://zhuanlan.zhihu.com/p/171627918

## HPO

1. 贝叶斯优化: 一种更好的超参数调优方式 - tobe的文章 - 知乎 https://zhuanlan.zhihu.com/p/29779000

## AutoML
1. 走马观花AutoML - 字节的文章 - 知乎
https://zhuanlan.zhihu.com/p/212512984





