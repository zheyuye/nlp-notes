# Optimization

## 非凸优化

* 非凸优化基石：Lipschitz Condition - Zeap的文章 - 知乎 https://zhuanlan.zhihu.com/p/27554191

## 优化器差异

* https://www.fast.ai/2018/07/02/adam-weight-decay/
* 深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 余昌黔的文章 - 知乎 https://zhuanlan.zhihu.com/p/22252270
* 当前训练神经网络最快的方式：AdamW优化算法+超级收敛 - 机器之心的文章 - 知乎 https://zhuanlan.zhihu.com/p/38945390
* 都9102年了，别再用Adam + L2 regularization了 - paperplanet的文章 - 知乎 https://zhuanlan.zhihu.com/p/63982470
* 关于weight_decay的深度分析 - Jack Sigmoid的文章 - 知乎 https://zhuanlan.zhihu.com/p/339448370
* [Why is it important to include a bias correction term for the Adam optimizer for Deep Learning?](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)

## 损失函数

* Focal loss论文详解 - 逍遥王可爱的文章 - 知乎 https://zhuanlan.zhihu.com/p/49981234
* [Focal Loss for Dense Object Detection in PyTorch](https://github.com/clcarwin/focal_loss_pytorch)
* triplet loss 损失函数 - 洪雨的文章 - 知乎 https://zhuanlan.zhihu.com/p/171627918
* [Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/triplet-loss)

## HPO

1. 贝叶斯优化: 一种更好的超参数调优方式 - tobe的文章 - 知乎 https://zhuanlan.zhihu.com/p/29779000

## AutoML
1. 走马观花AutoML - 字节的文章 - 知乎
https://zhuanlan.zhihu.com/p/212512984





