# Attention&Transformer

## Attention

1. 为什么Transformer 需要进行 Multi-head Attention？ - 香侬科技的回答 - 知乎 https://www.zhihu.com/question/341222779/answer/814111138
2. More About Attention - 李新春的文章 - 知乎 https://zhuanlan.zhihu.com/p/106662375
3. 模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 - 深度学习于NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/31547842
4. [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
5. [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/#citation)
6. [The Story of Heads](https://lena-voita.github.io/posts/acl19_heads.html)
7. 

## Basic

1. Transformer和Bert相关知识解答 - sliderSun的文章 - 知乎 https://zhuanlan.zhihu.com/p/149634836
2. 详解Transformer （Attention Is All You Need） - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/48508221
3. 【NLP】Transformer模型原理详解 - 李rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/44121378
4. **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)**
5. **[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)**
6. **[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)**

## PTMs

1. XLNet:运行机制及和Bert的异同比较 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/70257427

2. T5 模型：NLP Text-to-Text 预训练模型超大规模探索 - Andy Yang的文章 - 知乎 https://zhuanlan.zhihu.com/p/88438851

3. [Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)

4. [A Deep Dive into the Reformer](https://www.pragmatic.ml/reformer-deep-dive/)

## Model Compressing 

1. [Pruning BERT to accelerate inference](https://rasa.com/blog/pruning-bert-to-accelerate-inference/)
2. [Compressing BERT for faster prediction](https://rasa.com/blog/compressing-bert-for-faster-prediction-2/)

## Survey

1. **[A Survey of Long-Term Context in Transformers](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/)**
2. **[All The Ways You Can Compress BERT](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html)**



