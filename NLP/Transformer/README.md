

## Attention

1. 为什么Transformer 需要进行 Multi-head Attention？ - 香侬科技的回答 - 知乎 https://www.zhihu.com/question/341222779/answer/814111138
2. More About Attention - 李新春的文章 - 知乎 https://zhuanlan.zhihu.com/p/106662375
3. 模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 - 深度学习于NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/31547842

## Basic

Transformer和Bert相关知识解答 - sliderSun的文章 - 知乎 https://zhuanlan.zhihu.com/p/149634836

详解Transformer （Attention Is All You Need） - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/48508221

【NLP】Transformer模型原理详解 - 李rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/44121378

## PTMs

1. XLNet:运行机制及和Bert的异同比较 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/70257427
2. T5 模型：NLP Text-to-Text 预训练模型超大规模探索 - Andy Yang的文章 - 知乎 https://zhuanlan.zhihu.com/p/88438851